{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 The Riva Contact Application\n",
    "## (part of Lab 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll launch the Riva contact app with default ASR and NER (named entity recognition) services that you can try out for yourself!\n",
    "\n",
    "<img src=\"images/asr/contact-app-default.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6.1 Riva Contact](#6.1-Riva-Contact)<br>**\n",
    "**[6.2 Start the Riva ASR/NLP Services](#6.2-Start-the-Riva-ASR/NLP-Services)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.2.1 Exercise: Configure Riva for Streaming ASR and NER](#6.2.1-Exercise:-Configure-Riva-for-Streaming-ASR-and-NER)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.2.2 Start Riva Services](#6.2.2-Start-Riva-Services)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.2.3 Riva Available Services Check](#6.2.3-Riva-Available-Services-Check)<br>\n",
    "**[6.3 Run the Application Service](#6.3-Run-the-Application-Service)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.3.1 Start the Contact Web Server](#6.3.1-Start-the-Contact-Web-Server)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.3.2 Direct NER](#6.3.2-Direct-NER)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[6.3.3 Stop Riva Services](#6.3.3-Stop-Riva-Services)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Dependencies\n",
    "To run this app on your system, you will need:\n",
    "1. **Microphone**<br>\n",
    "For best ASR results, a headset is recommended.  \n",
    "1. **Chrome browser**<br>\n",
    "In order to use the app over HTTP in our class setup, you will need to override the browser block to your camera and microphone.  Instructions are included later in the notebook.\n",
    "1. **NGC Credentials**<br>Be sure you have added your NGC credential as described in the [NGC Setup notebook](003_Intro_NGC_Setup.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "# Check running docker containers. This should be empty.\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"docker kill\" requires at least 1 argument.\n",
      "See 'docker kill --help'.\n",
      "\n",
      "Usage:  docker kill [OPTIONS] CONTAINER [CONTAINER...]\n",
      "\n",
      "Kill one or more running containers\n",
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "# If not empty,\n",
    "# Clear Docker containers to start fresh...\n",
    "!docker kill $(docker ps -q)\n",
    "# Check for clean environment - this should be empty\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6.1 Riva Contact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Riva provides sample applications showing several use cases for virtual assistants and call centers. More about those samples can be found in the [Riva samples documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/). \n",
    "\n",
    "\n",
    "Riva Contact (from the [Riva Contact Center Video Conference sample](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/samples/callcenter.html)), is a web-based demonstration application for contact applications powered by conversational AI technologies. \n",
    "It is a peer-to-peer video conferencing app using streaming ASR and NLP.\n",
    "The application is based on a lightweight Node.js and backed by the robust NVIDIA Riva AI Services.\n",
    "\n",
    "In the background, each user’s web client sends a separate audio stream to the Riva Contact server. The server makes a streaming gRPC call to hosted Riva AI Services, which return an ongoing stream of ASR transcripts. This stream of transcripts is handed back to the speaker’s web client, with in-progress results that may change as the user speaks.\n",
    "\n",
    "When ASR results are marked as \"final\" (typically during short pauses in speech), the server hands the resulting transcript over to the NLP service for named entity recognition (NER). If Riva is configured to use a general-domain NER model, the service will recognize entities like the name of a person, location, or organization. \n",
    "\n",
    "Once the NER results are complete, the application server returns the final transcript and its NER annotation back to the web client. The web client then exchanges transcripts with the other user for an ongoing, annotated transcript of the conversation.  \n",
    "\n",
    "<img src=\"images/asr/riva-contact-architecture.png\" width=1000>\n",
    "\n",
    "Riva Contact is a Node.js application, intended to run in a Linux environment. It requires Riva Speech Services to be running with two primary models:\n",
    "- Streaming ASR\n",
    "- Named Entity Recognition (NER)\n",
    "\n",
    "You can use the default ASR and NER models available in `riva/models`.  Alternatively, you can deploy your own trained custom models for a specific domain - something you will try for yourself in a later notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6.2 Start the Riva ASR/NLP Services\n",
    "To deploy Riva AI Services, we can use the Riva Quick Start scripts as we did in the earlier [ASR deployment notebook](005_ASR_Riva_Deployment.ipynb), to set up a local workspace and deploy the Riva services using Docker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the workspace path to \"/path/to/your/workspace\"\n",
    "WORKSPACE = \"/dli/task\"\n",
    "\n",
    "# Set the location of the models directory\n",
    "RIVA_MODEL_LOC = WORKSPACE + \"/riva\"\n",
    "\n",
    "# Set the Riva Quick Start directory\n",
    "RIVA_QS = WORKSPACE + \"/riva/riva_quickstart\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Riva Quick Start configuration offers a list of available ASR, NLP and TTS models that can be downloaded from NGC and optimized for the target hardware. \n",
    "This process can take several minutes.  To save time, _the default Riva models have already been downloaded and optimized for the platform in `riva/models`_. \n",
    "If you wish to download any of these on your own system, you can uncomment the desired models in the Riva `config.sh` file and they can be downloaded using the [`riva_init.sh` command](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/custom-model-deployment.html?highlight=riva_init%20sh). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denoiser\n",
      "quartznet-asr-trt-ensemble-vad-streaming\n",
      "quartznet-asr-trt-ensemble-vad-streaming-ctc-decoder-cpu-streaming\n",
      "quartznet-asr-trt-ensemble-vad-streaming-feature-extractor-streaming\n",
      "quartznet-asr-trt-ensemble-vad-streaming-offline\n",
      "quartznet-asr-trt-ensemble-vad-streaming-offline-ctc-decoder-cpu-streaming-offline\n",
      "quartznet-asr-trt-ensemble-vad-streaming-offline-feature-extractor-streaming-offline\n",
      "quartznet-asr-trt-ensemble-vad-streaming-offline-voice-activity-detector-ctc-streaming-offline\n",
      "quartznet-asr-trt-ensemble-vad-streaming-voice-activity-detector-ctc-streaming\n",
      "riva-trt-quartznet\n",
      "riva-trt-riva_intent_weather-nn-bert-base-uncased\n",
      "riva-trt-riva_ner-nn-bert-base-uncased\n",
      "riva-trt-riva_punctuation-nn-bert-base-uncased\n",
      "riva-trt-riva_qa-nn-bert-base-uncased\n",
      "riva-trt-riva_text_classification_domain-nn-bert-base-uncased\n",
      "riva-trt-tacotron2_encoder\n",
      "riva-trt-waveglow\n",
      "riva_detokenize\n",
      "riva_intent_weather\n",
      "riva_label_tokens_weather\n",
      "riva_ner\n",
      "riva_ner_label_tokens\n",
      "riva_punctuation\n",
      "riva_punctuation_gen_output\n",
      "riva_punctuation_label_tokens_cap\n",
      "riva_punctuation_label_tokens_punct\n",
      "riva_punctuation_merge_labels\n",
      "riva_qa\n",
      "riva_qa_postprocessor\n",
      "riva_qa_preprocessor\n",
      "riva_text_classification_domain\n",
      "riva_tokenizer\n",
      "tacotron2_decoder_postnet\n",
      "tacotron2_ensemble\n",
      "tts_preprocessor\n"
     ]
    }
   ],
   "source": [
    "# Check available default riva models \n",
    "!ls $RIVA_MODEL_LOC/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the models are already downloaded and optimized, we don't need to run the `riva_init.sh` script and can move straight to starting the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1 Exercise: Configure Riva for Streaming ASR and NER\n",
    "To expose Riva ASR and NLP services, modify [config.sh](riva/riva_quickstart/config.sh) to \n",
    "- Enable ASR and NLP Riva services\n",
    "- Provide the encryption key (should already be correct)\n",
    "- Specify the correct model repository path to `riva_model_loc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your work against the [solution](solutions/ex6.2.1.sh) before moving on to the next section.  You can verify it with `diff` in the next cell. You should get no \"difference\" (an empty output) if your config file matches the solution.  \n",
    "\n",
    "Note that it would generally be necessary to also uncomment the models needed from NGC for streaming ASR and NER in the configuration file.  In this lab, it is not necessary because they've already been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43c43\n",
      "< riva_model_loc=\"/dli/task/riva/riva_quickstart/models_repo\"\n",
      "---\n",
      "> riva_model_loc=\"/dli/task/riva\"\n"
     ]
    }
   ],
   "source": [
    "# TODO modify config.sh so that this cell verifies changes are correct\n",
    "# There should be no output if the files match\n",
    "!diff $RIVA_QS/config.sh solutions/ex6.2.1.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2 Start Riva Services\n",
    "\n",
    "Now, we are ready to start the Riva server with ASR and NLP services. We'll first initialize with `riva_init.sh`.  The `riva_start.sh` script starts the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts.\n",
    "!cd $RIVA_QS && chmod +x *.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging into NGC docker registry if necessary...\n",
      "Pulling required docker images if necessary...\n",
      "Note: This may take some time, depending on the speed of your Internet connection.\n",
      "> Pulling Riva Speech Server images.\n",
      "  > Image nvcr.io/nvidia/riva/riva-speech:1.4.0-beta-server exists. Skipping.\n",
      "  > Image nvcr.io/nvidia/riva/riva-speech-client:1.4.0-beta exists. Skipping.\n",
      "  > Image nvcr.io/nvidia/riva/riva-speech:1.4.0-beta-servicemaker exists. Skipping.\n",
      "\n",
      "Downloading models (RMIRs) from NGC...\n",
      "Note: this may take some time, depending on the speed of your Internet connection.\n",
      "To skip this process and use existing RMIRs set the location and corresponding flag in config.sh.\n",
      "\n",
      "==========================\n",
      "=== Riva Speech Skills ===\n",
      "==========================\n",
      "\n",
      "NVIDIA Release devel (build 22382700)\n",
      "\n",
      "Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying\n",
      "project or file.\n",
      "\n",
      "NOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n",
      "   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "/data/artifacts /opt/riva\n",
      "/opt/riva\n",
      "\n",
      "Converting RMIRs at /dli/task/riva/riva_quickstart/models_repo/rmir to Riva Model repository.\n",
      "+ docker run --init -it --rm --gpus '\"device=0\"' -v /dli/task/riva/riva_quickstart/models_repo:/data -e MODEL_DEPLOY_KEY=tlt_encode --name riva-service-maker nvcr.io/nvidia/riva/riva-speech:1.4.0-beta-servicemaker deploy_all_models /data/rmir /data/models\n",
      "\n",
      "==========================\n",
      "=== Riva Speech Skills ===\n",
      "==========================\n",
      "\n",
      "NVIDIA Release devel (build 22382700)\n",
      "\n",
      "Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying\n",
      "project or file.\n",
      "\n",
      "NOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n",
      "   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "2022-03-13 20:15:40,451 [INFO] Writing Riva model repository to '/data/models'...\n",
      "2022-03-13 20:15:40,451 [INFO] The riva model repo target directory is /data/models\n",
      "2022-03-13 20:15:40,693 [WARNING] /data/models/riva-asr-feature-extractor-streaming already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva-asr-feature-extractor-streaming\n",
      "2022-03-13 20:15:40,693 [WARNING] /data/models/riva-trt-riva-asr-am-streaming already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva-trt-riva-asr-am-streaming\n",
      "2022-03-13 20:15:40,693 [WARNING] /data/models/riva-asr-voice-activity-detector-ctc-streaming already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva-asr-voice-activity-detector-ctc-streaming\n",
      "2022-03-13 20:15:40,693 [WARNING] /data/models/riva-asr-ctc-decoder-cpu-streaming already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva-asr-ctc-decoder-cpu-streaming\n",
      "2022-03-13 20:15:40,693 [WARNING] /data/models/riva-asr already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva-asr\n",
      "+ echo\n",
      "\n",
      "+ echo 'Riva initialization complete. Run ./riva_start.sh to launch services.'\n",
      "Riva initialization complete. Run ./riva_start.sh to launch services.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Riva\n",
    "!cd $RIVA_QS && bash riva_init.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Riva Speech Services. This may take several minutes depending on the number of models deployed.\n",
      "Waiting for Riva server to load all models...retrying in 10 seconds\n",
      "Waiting for Riva server to load all models...retrying in 10 seconds\n",
      "Riva server is ready...\n"
     ]
    }
   ],
   "source": [
    "# Start the Riva server\n",
    "!cd $RIVA_QS && bash riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Riva ASR and NLP services are running when you get \"Riva server is ready...\" (about 40 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.3 Riva Available Services Check\n",
    "\n",
    "To check the exposed Riva services, execute the `docker logs riva-speech` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "=== Riva Speech Skills ===\n",
      "==========================\n",
      "\n",
      "NVIDIA Release 21.07 (build 25292380)\n",
      "\n",
      "Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying\n",
      "project or file.\n",
      "\n",
      "NOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n",
      "   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "I0313 20:15:52.215767 73 metrics.cc:228] Collecting metrics for GPU 0: Tesla T4\n",
      "I0313 20:15:52.219368 73 onnxruntime.cc:1722] TRITONBACKEND_Initialize: onnxruntime\n",
      "I0313 20:15:52.219394 73 onnxruntime.cc:1732] Triton TRITONBACKEND API version: 1.0\n",
      "I0313 20:15:52.219400 73 onnxruntime.cc:1738] 'onnxruntime' TRITONBACKEND API version: 1.0\n",
      "I0313 20:15:52.408059 73 pinned_memory_manager.cc:206] Pinned memory pool is created at '0x7f5620000000' with size 268435456\n",
      "I0313 20:15:52.408447 73 cuda_memory_manager.cc:103] CUDA memory pool is created on device 0 with size 1000000000\n",
      "I0313 20:15:52.412813 73 model_repository_manager.cc:1066] loading: riva-asr-feature-extractor-streaming:1\n",
      "I0313 20:15:52.513122 73 model_repository_manager.cc:1066] loading: riva-asr-ctc-decoder-cpu-streaming:1\n",
      "I0313 20:15:52.513559 73 custom_backend.cc:201] Creating instance riva-asr-feature-extractor-streaming_0_0_gpu0 on GPU 0 (7.5) using libtriton_riva_asr_features.so\n",
      "I0313 20:15:52.613504 73 model_repository_manager.cc:1066] loading: riva-asr-voice-activity-detector-ctc-streaming:1\n",
      "I0313 20:15:52.613783 73 custom_backend.cc:198] Creating instance riva-asr-ctc-decoder-cpu-streaming_0_0_cpu on CPU using libtriton_riva_asr_decoder_cpu.so\n",
      "W:parameter_parser.cc:106: Parameter forerunner_start_offset_ms could not be set from parameters\n",
      "W:parameter_parser.cc:107: Default value will be used\n",
      "W:parameter_parser.cc:106: Parameter voc_string could not be set from parameters\n",
      "W:parameter_parser.cc:107: Default value will be used\n",
      "I0313 20:15:52.624281 73 model_repository_manager.cc:1240] successfully loaded 'riva-asr-ctc-decoder-cpu-streaming' version 1\n",
      "I0313 20:15:52.713825 73 model_repository_manager.cc:1066] loading: riva-trt-riva-asr-am-streaming:1\n",
      "I0313 20:15:52.714078 73 custom_backend.cc:198] Creating instance riva-asr-voice-activity-detector-ctc-streaming_0_0_cpu on CPU using libtriton_riva_asr_vad.so\n",
      "I0313 20:15:52.721131 73 model_repository_manager.cc:1240] successfully loaded 'riva-asr-voice-activity-detector-ctc-streaming' version 1\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "I0313 20:16:06.024232 73 model_repository_manager.cc:1240] successfully loaded 'riva-asr-feature-extractor-streaming' version 1\n",
      "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
      "I0313 20:16:06.716701 73 plan_backend.cc:384] Creating instance riva-trt-riva-asr-am-streaming_0_0_gpu0 on GPU 0 (7.5) using model.plan\n",
      "I0313 20:16:06.993398 73 plan_backend.cc:768] Created instance riva-trt-riva-asr-am-streaming_0_0_gpu0 on GPU 0 with stream priority 0 and optimization profile default[0];\n",
      "I0313 20:16:06.996190 73 model_repository_manager.cc:1240] successfully loaded 'riva-trt-riva-asr-am-streaming' version 1\n",
      "I0313 20:16:06.997035 73 model_repository_manager.cc:1066] loading: riva-asr:1\n",
      "I0313 20:16:07.097539 73 model_repository_manager.cc:1240] successfully loaded 'riva-asr' version 1\n",
      "I0313 20:16:07.097665 73 server.cc:504] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0313 20:16:07.097731 73 server.cc:543] \n",
      "+-------------+-----------------------------------------------------------------+--------+\n",
      "| Backend     | Path                                                            | Config |\n",
      "+-------------+-----------------------------------------------------------------+--------+\n",
      "| tensorrt    | <built-in>                                                      | {}     |\n",
      "| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so | {}     |\n",
      "+-------------+-----------------------------------------------------------------+--------+\n",
      "\n",
      "I0313 20:16:07.097788 73 server.cc:586] \n",
      "+------------------------------------------------+---------+--------+\n",
      "| Model                                          | Version | Status |\n",
      "+------------------------------------------------+---------+--------+\n",
      "| riva-asr                                       | 1       | READY  |\n",
      "| riva-asr-ctc-decoder-cpu-streaming             | 1       | READY  |\n",
      "| riva-asr-feature-extractor-streaming           | 1       | READY  |\n",
      "| riva-asr-voice-activity-detector-ctc-streaming | 1       | READY  |\n",
      "| riva-trt-riva-asr-am-streaming                 | 1       | READY  |\n",
      "+------------------------------------------------+---------+--------+\n",
      "\n",
      "I0313 20:16:07.097884 73 tritonserver.cc:1658] \n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Option                           | Value                                                                                                                                                                                  |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| server_id                        | triton                                                                                                                                                                                 |\n",
      "| server_version                   | 2.9.0                                                                                                                                                                                  |\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics |\n",
      "| model_repository_path[0]         | /data/models                                                                                                                                                                           |\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                              |\n",
      "| strict_model_config              | 1                                                                                                                                                                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                              |\n",
      "| cuda_memory_pool_byte_size{0}    | 1000000000                                                                                                                                                                             |\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                    |\n",
      "| strict_readiness                 | 1                                                                                                                                                                                      |\n",
      "| exit_timeout                     | 30                                                                                                                                                                                     |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "I0313 20:16:07.098979 73 grpc_server.cc:4028] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0313 20:16:07.099229 73 http_server.cc:2761] Started HTTPService at 0.0.0.0:8000\n",
      "I0313 20:16:07.140694 73 http_server.cc:2780] Started Metrics Service at 0.0.0.0:8002\n",
      "  > Triton server is ready...\n",
      "I0313 20:16:07.320516   169 grpc_health.cc:27] RivaHealthService initialized with server: localhost:8001\n",
      "I0313 20:16:07.320559   169 grpc_riva_asr.cc:148] Setting uri for ASRServiceImpl\n",
      "I0313 20:16:07.320564   169 grpc_riva_asr.cc:149] Initializing different models\n",
      "I0313 20:16:07.320899   169 model_registry.cc:36] RivaModelRegistry initialized with server: localhost:8001\n",
      "I0313 20:16:07.321769   169 model_registry.cc:65] Server Name: triton, Server version: 2.9.0\n",
      "I0313 20:16:07.321951   169 model_registry.cc:86] Our model repository has a total of: 5 models\n",
      "I0313 20:16:07.321964   169 model_registry.cc:91] Model names: riva-asr, Model version: 1\n",
      "I0313 20:16:07.324600   169 model_registry.cc:104] 'Successfully registering riva-asr'\n",
      "I0313 20:16:07.324645   169 model_registry.cc:91] Model names: riva-asr-ctc-decoder-cpu-streaming, Model version: 1\n",
      "I0313 20:16:07.325644   169 model_registry.cc:91] Model names: riva-asr-feature-extractor-streaming, Model version: 1\n",
      "I0313 20:16:07.326694   169 model_registry.cc:91] Model names: riva-asr-voice-activity-detector-ctc-streaming, Model version: 1\n",
      "I0313 20:16:07.327436   169 model_registry.cc:91] Model names: riva-trt-riva-asr-am-streaming, Model version: 1\n",
      "I0313 20:16:07.328042   169 model_registry.cc:109] Successfully registered: 1 models.\n",
      "I0313 20:16:07.328059   169 client.cc:38] RivaLanguageUnderstandingClient initialized with server: localhost:8001\n",
      "I0313 20:16:07.328290   169 client.cc:54] Our model repository has: 5 models.\n",
      "W0313 20:16:07.329200   169 client.cc:78] Registration of 'riva-asr' failed with unknown service type\n",
      "I0313 20:16:07.332074   169 grpc_riva_asr.cc:173] Punctuation model does not exist on server\n",
      "I0313 20:16:07.332087   169 grpc_riva_asr.cc:177] Seeding RNG used for correlation id with time: 1647202567\n",
      "I0313 20:16:07.365904   169 grpc_riva_asr.cc:148] Setting uri for ASRServiceImpl\n",
      "I0313 20:16:07.365928   169 grpc_riva_asr.cc:149] Initializing different models\n",
      "I0313 20:16:07.365936   169 model_registry.cc:36] RivaModelRegistry initialized with server: localhost:8001\n",
      "I0313 20:16:07.366397   169 model_registry.cc:65] Server Name: triton, Server version: 2.9.0\n",
      "I0313 20:16:07.366607   169 model_registry.cc:86] Our model repository has a total of: 5 models\n",
      "I0313 20:16:07.366619   169 model_registry.cc:91] Model names: riva-asr, Model version: 1\n",
      "I0313 20:16:07.367548   169 model_registry.cc:104] 'Successfully registering riva-asr'\n",
      "I0313 20:16:07.367594   169 model_registry.cc:91] Model names: riva-asr-ctc-decoder-cpu-streaming, Model version: 1\n",
      "I0313 20:16:07.368463   169 model_registry.cc:91] Model names: riva-asr-feature-extractor-streaming, Model version: 1\n",
      "I0313 20:16:07.369401   169 model_registry.cc:91] Model names: riva-asr-voice-activity-detector-ctc-streaming, Model version: 1\n",
      "I0313 20:16:07.370133   169 model_registry.cc:91] Model names: riva-trt-riva-asr-am-streaming, Model version: 1\n",
      "I0313 20:16:07.370790   169 model_registry.cc:109] Successfully registered: 1 models.\n",
      "I0313 20:16:07.370808   169 client.cc:38] RivaLanguageUnderstandingClient initialized with server: localhost:8001\n",
      "I0313 20:16:07.371013   169 client.cc:54] Our model repository has: 5 models.\n",
      "W0313 20:16:07.371824   169 client.cc:78] Registration of 'riva-asr' failed with unknown service type\n",
      "I0313 20:16:07.374624   169 grpc_riva_asr.cc:173] Punctuation model does not exist on server\n",
      "I0313 20:16:07.374637   169 grpc_riva_asr.cc:177] Seeding RNG used for correlation id with time: 1647202567\n",
      "I0313 20:16:07.405063   169 client.cc:38] RivaLanguageUnderstandingClient initialized with server: localhost:8001\n",
      "I0313 20:16:07.405428   169 client.cc:54] Our model repository has: 5 models.\n",
      "W0313 20:16:07.406425   169 client.cc:78] Registration of 'riva-asr' failed with unknown service type\n",
      "I0313 20:16:07.409353   169 model_registry.cc:36] RivaModelRegistry initialized with server: localhost:8001\n",
      "I0313 20:16:07.409655   169 model_registry.cc:65] Server Name: triton, Server version: 2.9.0\n",
      "I0313 20:16:07.409853   169 model_registry.cc:86] Our model repository has a total of: 5 models\n",
      "I0313 20:16:07.409863   169 model_registry.cc:91] Model names: riva-asr, Model version: 1\n",
      "I0313 20:16:07.410722   169 model_registry.cc:104] 'Successfully registering riva-asr'\n",
      "I0313 20:16:07.410775   169 model_registry.cc:91] Model names: riva-asr-ctc-decoder-cpu-streaming, Model version: 1\n",
      "I0313 20:16:07.411594   169 model_registry.cc:91] Model names: riva-asr-feature-extractor-streaming, Model version: 1\n",
      "I0313 20:16:07.412492   169 model_registry.cc:91] Model names: riva-asr-voice-activity-detector-ctc-streaming, Model version: 1\n",
      "I0313 20:16:07.413197   169 model_registry.cc:91] Model names: riva-trt-riva-asr-am-streaming, Model version: 1\n",
      "I0313 20:16:07.413779   169 model_registry.cc:109] Successfully registered: 1 models.\n",
      "I0313 20:16:07.413790   169 grpc_riva_nlp.cc:33] NLPService GRPC service started\n",
      "I0313 20:16:07.413796   169 client.cc:38] RivaLanguageUnderstandingClient initialized with server: localhost:8001\n",
      "I0313 20:16:07.413997   169 client.cc:54] Our model repository has: 5 models.\n",
      "W0313 20:16:07.414815   169 client.cc:78] Registration of 'riva-asr' failed with unknown service type\n",
      "I0313 20:16:07.417580   169 model_registry.cc:36] RivaModelRegistry initialized with server: localhost:8001\n",
      "I0313 20:16:07.417893   169 model_registry.cc:65] Server Name: triton, Server version: 2.9.0\n",
      "I0313 20:16:07.418092   169 model_registry.cc:86] Our model repository has a total of: 5 models\n",
      "I0313 20:16:07.418102   169 model_registry.cc:91] Model names: riva-asr, Model version: 1\n",
      "I0313 20:16:07.418928   169 model_registry.cc:104] 'Successfully registering riva-asr'\n",
      "I0313 20:16:07.418973   169 model_registry.cc:91] Model names: riva-asr-ctc-decoder-cpu-streaming, Model version: 1\n",
      "I0313 20:16:07.419775   169 model_registry.cc:91] Model names: riva-asr-feature-extractor-streaming, Model version: 1\n",
      "I0313 20:16:07.420670   169 model_registry.cc:91] Model names: riva-asr-voice-activity-detector-ctc-streaming, Model version: 1\n",
      "I0313 20:16:07.421451   169 model_registry.cc:91] Model names: riva-trt-riva-asr-am-streaming, Model version: 1\n",
      "I0313 20:16:07.422039   169 model_registry.cc:109] Successfully registered: 1 models.\n",
      "I0313 20:16:07.422050   169 grpc_riva_nlp.cc:33] NLPService GRPC service started\n",
      "I0313 20:16:07.422068   169 client.cc:38] RivaLanguageUnderstandingClient initialized with server: localhost:8001\n",
      "I0313 20:16:07.422272   169 client.cc:54] Our model repository has: 5 models.\n",
      "W0313 20:16:07.423106   169 client.cc:78] Registration of 'riva-asr' failed with unknown service type\n",
      "I0313 20:16:07.426074   169 model_registry.cc:36] RivaModelRegistry initialized with server: localhost:8001\n",
      "I0313 20:16:07.426396   169 model_registry.cc:65] Server Name: triton, Server version: 2.9.0\n",
      "I0313 20:16:07.426542   169 model_registry.cc:86] Our model repository has a total of: 5 models\n",
      "I0313 20:16:07.426553   169 model_registry.cc:91] Model names: riva-asr, Model version: 1\n",
      "I0313 20:16:07.427383   169 model_registry.cc:104] 'Successfully registering riva-asr'\n",
      "I0313 20:16:07.427433   169 model_registry.cc:91] Model names: riva-asr-ctc-decoder-cpu-streaming, Model version: 1\n",
      "I0313 20:16:07.428243   169 model_registry.cc:91] Model names: riva-asr-feature-extractor-streaming, Model version: 1\n",
      "I0313 20:16:07.429128   169 model_registry.cc:91] Model names: riva-asr-voice-activity-detector-ctc-streaming, Model version: 1\n",
      "I0313 20:16:07.429845   169 model_registry.cc:91] Model names: riva-trt-riva-asr-am-streaming, Model version: 1\n",
      "I0313 20:16:07.430445   169 model_registry.cc:109] Successfully registered: 1 models.\n",
      "I0313 20:16:07.430462   169 grpc_riva_nlp.cc:33] NLPService GRPC service started\n",
      "I0313 20:16:07.430739   169 riva_server.cc:91] NLP Service connected to Triton at localhost:8001\n",
      "I0313 20:16:07.430750   169 riva_server.cc:93] ASR Service connected to Triton at localhost:8001\n",
      "I0313 20:16:07.430753   169 riva_server.cc:96] Riva Conversational AI Server listening on 0.0.0.0:50051\n"
     ]
    }
   ],
   "source": [
    "!docker logs riva-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6.3 Run the Application Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the app, we use Riva for two purposes:\n",
    "1. To get a streaming transcript of the conversation using ASR\n",
    "1. To tag key phrases (named entities) in that transcript with NER. \n",
    "\n",
    "The flow is:\n",
    "- Extract an audio stream from the client and pass that audio to the Node.js server (step 1).\n",
    "- The Node.js server calls Riva services, using gRPC, to get transcripts (step 2) and named entities (step 4)\n",
    "- The Node.js server sends the results back to the client (step 3 and 5).\n",
    "- The client can then render the transcripts in the browser and pass the transcripts over the peer-to-peer connection so that both users can see the whole conversation.\n",
    "\n",
    "Riva Contact uses environment variables to manage its configuration parameters. These are kept in a configuration file, [env.txt](contact-app/env.txt). In your own project, you may wish to change some of these configurations.  For example, the NER entity list would change if you were to deploy your own custom domain-specific NER model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.1 Start the Contact Web Server\n",
    "To start the web service, open a JupyterLab terminal.  You can do this by first opening the JupyterLab Launcher (small '+' sign at the top of the file browser) and clicking the \"Terminal\" icon.  Next, enter the following in the terminal to start the app server:  \n",
    "\n",
    "```sh\n",
    "cd /dli/task/contact-app\n",
    "npm install\n",
    "npm run start\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"\", data-commandlinker-command=\"terminal:create-new\">Open Terminal</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell, then click the link to open a terminal\n",
    "# Enter the commands provided above in the terminal window to start the web server\n",
    "from IPython.display import HTML\n",
    "HTML('<a href=\"\", data-commandlinker-command=\"terminal:create-new\">Open Terminal</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the terminal window, you should see that the server has started running on port 8009:\n",
    "\n",
    "<img src=\"images/asr/webserver_running.png\">\n",
    "\n",
    "After you have started the server, execute the following cell to create a link to open the app! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "const href = window.location.hostname + '/app/';\n",
       "let a = document.createElement('a');\n",
       "let link = document.createTextNode('Open Riva Contact!');\n",
       "a.appendChild(link);\n",
       "a.href = \"http://\" + href;\n",
       "a.style.color = \"navy\"\n",
       "a.target = \"_blank\"\n",
       "element.append(a);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "const href = window.location.hostname + '/app/';\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Riva Contact!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href;\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you open the app, you may see a \"Lost connection to server\" alert.  Just click \"Ok\".  Other than that, your initial view should look like the following:\n",
    "\n",
    "<img src=\"images/asr/riva_contact_start.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> ***WARNING Browser Restrictions WARNING***</h3>\n",
    "\n",
    "To use the web app, access to your microphone is required (camera is optional).<br>\n",
    "Several browsers restrict camera/microphone access to applications served from a secure origin (HTTPS or local IP).\n",
    "For your own development purposes, you can set up a self-signed certificate or proxy.  \n",
    "\n",
    "Some browsers provide a way to treat specific URLs as secure:\n",
    "- Chrome browser: <br>\n",
    "Configure the \"treat insecure origin as secure\" flag by adding the application URL on the following page: <br>\n",
    "   ***chrome://flags/#unsafely-treat-insecure-origin-as-secure***<br>\n",
    "(Copy and paste this \"chrome://\" link to a tab on your browser to open the page) <br>\n",
    "You'll see the flag with a text window at the top of the page.  Add your own course URL to the box.  Here is an example (your URL is different).<br>\n",
    "More discussion can be found in [this blog](https://medium.com/@Carmichaelize/enabling-the-microphone-camera-in-chrome-for-local-unsecure-origins-9c90c3149339).\n",
    "\n",
    "<img src=\"images/asr/chrome_override_example.png\">\n",
    "\n",
    "- Safari browser: <br>\n",
    "Enable in the menu: Develop > WebRTC > Allow Media Capture on Insecure Sites\n",
    "\n",
    "This will require you to restart the browser. No worries, nothing will be lost.\n",
    "\n",
    "Go to the Application URL. Once the page is loaded, you're welcome to start the Riva transcription.\n",
    "In the box titled \"Riva transcription,\" hit the \"Start\" button, then start speaking. You'll see in-progress transcripts in the text field at the bottom. As those transcripts are finalized, they'll appear, with NER annotations, in the transcription box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2 Direct NER \n",
    "\n",
    "It is also possible to call the NER service directly without speaking. Type into the text field at the bottom and hit the \"Submit\" button. This will directly call the NER capability exposed by Riva with the submitted text (without calling the ASR service)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.3 Stop Riva Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down Riva \n",
    "!bash $RIVA_QS/riva_stop.sh\n",
    "# Shut down web app\n",
    "!pkill -9 node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you have:\n",
    "- Started ASR and NER default Riva services\n",
    "- Launched the Riva Contact app \n",
    "- Demonstrated live streaming ASR with NER for yourself!\n",
    "\n",
    "This concludes the ASR portion of the course.  Next, you'll start a deeper dive into NLP services, beginning with the [NER fine-tuning notebook](007_NLP_Finetune_NER.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
