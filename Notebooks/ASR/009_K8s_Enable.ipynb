{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e38888",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f059dc4",
   "metadata": {},
   "source": [
    "# 9.0 Enabling GPU within a Kubernetes (K8s) Cluster\n",
    "## (part of Lab 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f659cd0",
   "metadata": {},
   "source": [
    "<img src=\"images/k8s/kubernetes_stack_0.png\" style=\"float: right;\">\n",
    "In this notebook, you'll learn how to prepare a Kubernetes cluster for GPU acceleration full production deployment of conversational AI applications.<br><br>\n",
    "\n",
    "**[9.1 Launch a K8s Cluster](#9.1-Launch-a-K8s-Cluster)<br>**\n",
    "**[9.2 Deploy a CUDA Test Application](#9.2-Deploy-a-CUDA-Test-Application)<br>**\n",
    "**[9.3 Add GPU Awareness to K8s](#9.3-Add-GPU-Awareness-to-K8s)<br>**\n",
    "**[9.4 Interact with GPU Resources in K8s](#9.4-Interact-with-GPU-Resources-in-K8s)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[9.4.1 Exercise: Configure Pod](#9.4.1-Exercise:-Configure-Pod)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[9.4.2 Final Checks and Shutdown](#9.4.2-Final-Checks-and-Shutdown)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[9.4.2.1 Exercise: Delete a Pod](#9.4.2.1-Exercise:-Delete-a-Pod)<br>\n",
    "\n",
    "In the previous parts of the class, you deployed NVIDIA Riva using basic shell commands. As convenient as this method is during development, it becomes impractical when deploying to production, that is, when managing larger numbers of servers and services. \n",
    "\n",
    "[Kubernetes](https://kubernetes.io/), also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. \n",
    "In this part of the class, we will first launch a K8s cluster, enable the cluster for GPU acceleration and interact with those resources. This is our first step toward monitoring, managing, and deploying conversational AI applications in production. Monitoring and deployment will be covered in later notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd220e69",
   "metadata": {},
   "source": [
    "### Notebook Dependencies\n",
    "The steps in this notebook assume that you are starting with a clean environment.  Ensure that by stopping any previous Kubernetes installation and all docker containers, then looking at our environment's state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137b5d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "# Check running docker containers. This should be empty.\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54854d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"docker kill\" requires at least 1 argument.\n",
      "See 'docker kill --help'.\n",
      "\n",
      "Usage:  docker kill [OPTIONS] CONTAINER [CONTAINER...]\n",
      "\n",
      "Kill one or more running containers\n",
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "# If not empty,\n",
    "# Clear Docker containers to start fresh...\n",
    "!docker kill $(docker ps -q)\n",
    "\n",
    "# Check for clean environment - this should be empty\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5f863c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🙄  \"minikube\" profile does not exist, trying anyways.\n",
      "💀  Removed all traces of the \"minikube\" cluster.\n"
     ]
    }
   ],
   "source": [
    "# Deletes local Kubernetes cluster if it exists\n",
    "!minikube delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94886eb",
   "metadata": {},
   "source": [
    "--- \n",
    "# 9.1 Launch a K8s Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4ae00",
   "metadata": {},
   "source": [
    "A [Kubernetes cluster](https://kubernetes.io/docs/concepts/overview/components/) consists of a set of worker machines (physical or virtual), called nodes, that run containerized applications. Every cluster has at least one worker node, though it can also support thousands of nodes! For this class, we will use [Minikube](https://minikube.sigs.k8s.io/docs/), which allows us to deploy a local and self-contained Kubernetes cluster with a single node. \n",
    "\n",
    "Review the class hardware resources available and launch the K8s cluster.\n",
    "\n",
    "We can see details and status of the available GPU using the `nvidia-smi` command.\n",
    "\n",
    "<img src=\"images/k8s/nvidia_smi.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4852df9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 27 09:52:42 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   28C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# What GPU are we using and how much memory does it have?\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adb371ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name\t: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "model name\t: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "model name\t: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "model name\t: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n"
     ]
    }
   ],
   "source": [
    "# What type of CPU processor(s) are we using?\n",
    "!cat /proc/cpuinfo | grep \"model name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c64eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# How many processors are available?\n",
    "!nproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed892337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😄  minikube v1.19.0 on Ubuntu 20.04 (docker/amd64)\n",
      "✨  Using the none driver based on user configuration\n",
      "👍  Starting control plane node minikube in cluster minikube\n",
      "🤹  Running on localhost (CPUs=4, Memory=15717MB, Disk=297738MB) ...\n",
      "ℹ️  OS release is Ubuntu 20.04.1 LTS\n",
      "    > kubeadm.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K| \n",
      "    > kubectl.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s\n",
      "    > kubelet.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s\n",
      "    > kubeadm: 37.40 MiB / 37.40 MiB [-------------] 100.00% 1.87 GiB p/s 219ms\n",
      "    > kubectl: 38.37 MiB / 38.37 MiB [------------] 100.00% 12.91 GiB p/s 206ms\n",
      "    > kubelet: 108.73 MiB / 108.73 MiB [---------] 100.00% 320.44 MiB p/s 539msK\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K| \n",
      "\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\n",
      "    ▪ Generating certificates and keys ...\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\n",
      "    ▪ Booting up control plane ...\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\n",
      "    ▪ Configuring RBAC rules ...\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\n",
      "🤹  Configuring local host environment ...\n",
      "\n",
      "❗  The 'none' driver is designed for experts who need to integrate with an existing VM\n",
      "💡  Most users should use the newer 'docker' driver instead, which does not require root!\n",
      "📘  For more information, see: https://minikube.sigs.k8s.io/docs/reference/drivers/none/\n",
      "\n",
      "❗  kubectl and minikube configuration will be stored in /root\n",
      "❗  To use kubectl or minikube commands as your own user, you may need to relocate them. For example, to overwrite your own settings, run:\n",
      "\n",
      "    ▪ sudo mv /root/.kube /root/.minikube $HOME\n",
      "    ▪ sudo chown -R $USER $HOME/.kube $HOME/.minikube\n",
      "\n",
      "💡  This can also be done automatically by setting the env var CHANGE_MINIKUBE_NONE_USER=true\n",
      "🔎  Verifying Kubernetes components...\n",
      "    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5\n",
      "🌟  Enabled addons: default-storageclass, storage-provisioner\n",
      "🏄  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n"
     ]
    }
   ],
   "source": [
    "# Launch the K8s cluster using Minikube\n",
    "!minikube start --driver=none"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7dee1",
   "metadata": {},
   "source": [
    "Once the cluster is successfully launched, we expect to see a number of containers running.  Check this by executing `docker ps` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b296f82a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                     COMMAND                  CREATED          STATUS          PORTS     NAMES\n",
      "d8ce88e65dd1   gcr.io/k8s-minikube/storage-provisioner   \"/storage-provisioner\"   4 seconds ago    Up 3 seconds              k8s_storage-provisioner_storage-provisioner_kube-system_d60a76c3-475a-486f-ab1a-ef78d7f28437_0\n",
      "15e8c210c8e5   k8s.gcr.io/pause:3.2                      \"/pause\"                 5 seconds ago    Up 5 seconds              k8s_POD_storage-provisioner_kube-system_d60a76c3-475a-486f-ab1a-ef78d7f28437_0\n",
      "a78bfacd9623   bfe3a36ebd25                              \"/coredns -conf /etc…\"   8 seconds ago    Up 7 seconds              k8s_coredns_coredns-74ff55c5b-zv7v2_kube-system_830c4107-4186-4d70-a93b-d3b70c6b84cb_0\n",
      "068edd4a0048   43154ddb57a8                              \"/usr/local/bin/kube…\"   8 seconds ago    Up 7 seconds              k8s_kube-proxy_kube-proxy-n6m24_kube-system_6217ce95-7758-4d6f-abf8-d4218355eb21_0\n",
      "601c57a3f3ff   k8s.gcr.io/pause:3.2                      \"/pause\"                 8 seconds ago    Up 7 seconds              k8s_POD_coredns-74ff55c5b-zv7v2_kube-system_830c4107-4186-4d70-a93b-d3b70c6b84cb_0\n",
      "1b021ec22f45   k8s.gcr.io/pause:3.2                      \"/pause\"                 8 seconds ago    Up 7 seconds              k8s_POD_kube-proxy-n6m24_kube-system_6217ce95-7758-4d6f-abf8-d4218355eb21_0\n",
      "ad166d6395fb   0369cf4303ff                              \"etcd --advertise-cl…\"   31 seconds ago   Up 30 seconds             k8s_etcd_etcd-442be9fcde81_kube-system_ca4f5997866adfd00f1436cf7129ddbc_0\n",
      "1fa6ea9f9f61   a27166429d98                              \"kube-controller-man…\"   31 seconds ago   Up 30 seconds             k8s_kube-controller-manager_kube-controller-manager-442be9fcde81_kube-system_57b8c22dbe6410e4bd36cf14b0f8bdc7_0\n",
      "6479b084cd8e   ed2c44fbdd78                              \"kube-scheduler --au…\"   31 seconds ago   Up 30 seconds             k8s_kube-scheduler_kube-scheduler-442be9fcde81_kube-system_6b4a0ee8b3d15a1c2e47c15d32e6eb0d_0\n",
      "68d779437ea6   a8c2fdb8bf76                              \"kube-apiserver --ad…\"   31 seconds ago   Up 30 seconds             k8s_kube-apiserver_kube-apiserver-442be9fcde81_kube-system_970a9497bdad55b49ab2fa9a00c24866_0\n",
      "2b9bef6b71f8   k8s.gcr.io/pause:3.2                      \"/pause\"                 59 seconds ago   Up 30 seconds             k8s_POD_kube-controller-manager-442be9fcde81_kube-system_57b8c22dbe6410e4bd36cf14b0f8bdc7_0\n",
      "8feadf27c574   k8s.gcr.io/pause:3.2                      \"/pause\"                 59 seconds ago   Up 30 seconds             k8s_POD_kube-apiserver-442be9fcde81_kube-system_970a9497bdad55b49ab2fa9a00c24866_0\n",
      "251a3e914afd   k8s.gcr.io/pause:3.2                      \"/pause\"                 59 seconds ago   Up 30 seconds             k8s_POD_etcd-442be9fcde81_kube-system_ca4f5997866adfd00f1436cf7129ddbc_0\n",
      "30f328fb7117   k8s.gcr.io/pause:3.2                      \"/pause\"                 59 seconds ago   Up 30 seconds             k8s_POD_kube-scheduler-442be9fcde81_kube-system_6b4a0ee8b3d15a1c2e47c15d32e6eb0d_0\n"
     ]
    }
   ],
   "source": [
    "# Listing the Kuberenetes components deployed\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067782ac",
   "metadata": {},
   "source": [
    "We should now have access to the [kubectl command line tool](https://kubernetes.io/docs/reference/kubectl/overview/), which is used to interact with the cluster. List the nodes and services in the cluster using the `kubectl get` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e79f650e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           STATUS   ROLES                  AGE   VERSION\n",
      "442be9fcde81   Ready    control-plane,master   87s   v1.20.2\n"
     ]
    }
   ],
   "source": [
    "# List nodes in the cluster\n",
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "449df1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
      "kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   100s\n"
     ]
    }
   ],
   "source": [
    "# List all services deployed\n",
    "!kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b7ccbb",
   "metadata": {},
   "source": [
    "--- \n",
    "# 9.2 Deploy a CUDA Test Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70041aec",
   "metadata": {},
   "source": [
    "Next, we will deploy a simple GPU-accelerated application. This is a toy application which randomly generates two very large vectors and adds them. Print out the YAML configuration file needed to deploy the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f847675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the configuration directory\n",
    "CONFIG_DIR='/dli/task/kubernetes-config'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b367658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: gpu-operator-test\n",
      "spec:\n",
      "  restartPolicy: OnFailure\n",
      "  containers:\n",
      "  - name: cuda-vector-add\n",
      "    image: \"nvidia/samples:vectoradd-cuda10.2\"\n",
      "    resources:\n",
      "      limits:\n",
      "         nvidia.com/gpu: 1"
     ]
    }
   ],
   "source": [
    "# Review the application we will deploy\n",
    "!cat $CONFIG_DIR/gpu-pod.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa13754",
   "metadata": {},
   "source": [
    "The main difference between a YAML file specifying a GPU-accelerated application compared to one specifying a non-GPU-accelerated application, is the configuration of the GPU resources required. In our case, we have created a basic configuration requesting a single NVIDIA GPU by setting `resources: limits: nvidia.com/gpu:` to 1. \n",
    "\n",
    "To deploy an application, execute the `kubectl apply` command, specifying the YAML configuration file with the `-f` file option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a5181d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/gpu-operator-test created\n"
     ]
    }
   ],
   "source": [
    "# Deploy the application\n",
    "!kubectl apply -f $CONFIG_DIR/gpu-pod.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bbfc8",
   "metadata": {},
   "source": [
    "Once deployed, we can observe the status of a pod created with `kubectl get`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e90fa41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                READY   STATUS    RESTARTS   AGE\n",
      "gpu-operator-test   0/1     Pending   0          57s\n"
     ]
    }
   ],
   "source": [
    "# Get the status of the pod deployed\n",
    "!kubectl get pods gpu-operator-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1fa57a",
   "metadata": {},
   "source": [
    "At this stage, the application is in the \"Pending\" state. <br>\n",
    "Why do you think this is case? Do you think its just the fact we have not given the application enough time to launch? Or do you think there are other reasons for this behavior? Try executing the same command again to see if the status changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b220f010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                READY   STATUS    RESTARTS   AGE\n",
      "gpu-operator-test   0/1     Pending   0          73s\n"
     ]
    }
   ],
   "source": [
    "# Checking again. Is it still pending?\n",
    "!kubectl get pods gpu-operator-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e3a80",
   "metadata": {},
   "source": [
    "So the application is indeed in the \"Pending\" state and it will remain like that irrespective of the amount of time we wait. Why? Begin to answer this by looking at the configuration of the available nodes (in our case we just have one). In particular, look for any NVIDIA-specific configuration using the `kubectl describe` command, as this will help us identify GPU resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09d8f04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:               442be9fcde81\n",
      "Roles:              control-plane,master\n",
      "Labels:             beta.kubernetes.io/arch=amd64\n",
      "                    beta.kubernetes.io/os=linux\n",
      "                    kubernetes.io/arch=amd64\n",
      "                    kubernetes.io/hostname=442be9fcde81\n",
      "                    kubernetes.io/os=linux\n",
      "                    minikube.k8s.io/commit=15cede53bdc5fe242228853e737333b09d4336b5\n",
      "                    minikube.k8s.io/name=minikube\n",
      "                    minikube.k8s.io/updated_at=2022_04_27T10_44_30_0700\n",
      "                    minikube.k8s.io/version=v1.19.0\n",
      "                    node-role.kubernetes.io/control-plane=\n",
      "                    node-role.kubernetes.io/master=\n",
      "Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n",
      "                    node.alpha.kubernetes.io/ttl: 0\n",
      "                    volumes.kubernetes.io/controller-managed-attach-detach: true\n",
      "CreationTimestamp:  Wed, 27 Apr 2022 10:44:29 +0000\n",
      "Taints:             <none>\n",
      "Unschedulable:      false\n",
      "Lease:\n",
      "  HolderIdentity:  442be9fcde81\n",
      "  AcquireTime:     <unset>\n",
      "  RenewTime:       Wed, 27 Apr 2022 10:50:24 +0000\n",
      "Conditions:\n",
      "  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n",
      "  ----             ------  -----------------                 ------------------                ------                       -------\n",
      "  MemoryPressure   False   Wed, 27 Apr 2022 10:50:05 +0000   Wed, 27 Apr 2022 10:44:28 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n",
      "  DiskPressure     False   Wed, 27 Apr 2022 10:50:05 +0000   Wed, 27 Apr 2022 10:44:28 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n",
      "  PIDPressure      False   Wed, 27 Apr 2022 10:50:05 +0000   Wed, 27 Apr 2022 10:44:28 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n",
      "  Ready            True    Wed, 27 Apr 2022 10:50:05 +0000   Wed, 27 Apr 2022 10:44:44 +0000   KubeletReady                 kubelet is posting ready status\n",
      "Addresses:\n",
      "  InternalIP:  172.18.0.5\n",
      "  Hostname:    442be9fcde81\n",
      "Capacity:\n",
      "  cpu:                4\n",
      "  ephemeral-storage:  304884524Ki\n",
      "  hugepages-1Gi:      0\n",
      "  hugepages-2Mi:      0\n",
      "  memory:             16095212Ki\n",
      "  pods:               110\n",
      "Allocatable:\n",
      "  cpu:                4\n",
      "  ephemeral-storage:  304884524Ki\n",
      "  hugepages-1Gi:      0\n",
      "  hugepages-2Mi:      0\n",
      "  memory:             16095212Ki\n",
      "  pods:               110\n",
      "System Info:\n",
      "  Machine ID:                 84fb46bd39d2483a97ab4430ee4a5e3a\n",
      "  System UUID:                4138acd6-eadc-4c1a-8c42-4fbbc960846d\n",
      "  Boot ID:                    a3589ab0-527a-424b-aa63-1ab9fd222aca\n",
      "  Kernel Version:             5.4.0-1041-aws\n",
      "  OS Image:                   Ubuntu 20.04.1 LTS\n",
      "  Operating System:           linux\n",
      "  Architecture:               amd64\n",
      "  Container Runtime Version:  docker://20.10.3\n",
      "  Kubelet Version:            v1.20.2\n",
      "  Kube-Proxy Version:         v1.20.2\n",
      "PodCIDR:                      10.244.0.0/24\n",
      "PodCIDRs:                     10.244.0.0/24\n",
      "Non-terminated Pods:          (7 in total)\n",
      "  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n",
      "  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---\n",
      "  kube-system                 coredns-74ff55c5b-zv7v2                 100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     5m46s\n",
      "  kube-system                 etcd-442be9fcde81                       100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         5m57s\n",
      "  kube-system                 kube-apiserver-442be9fcde81             250m (6%)     0 (0%)      0 (0%)           0 (0%)         5m57s\n",
      "  kube-system                 kube-controller-manager-442be9fcde81    200m (5%)     0 (0%)      0 (0%)           0 (0%)         5m57s\n",
      "  kube-system                 kube-proxy-n6m24                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m46s\n",
      "  kube-system                 kube-scheduler-442be9fcde81             100m (2%)     0 (0%)      0 (0%)           0 (0%)         5m57s\n",
      "  kube-system                 storage-provisioner                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         6m\n",
      "Allocated resources:\n",
      "  (Total limits may be over 100 percent, i.e., overcommitted.)\n",
      "  Resource           Requests    Limits\n",
      "  --------           --------    ------\n",
      "  cpu                750m (18%)  0 (0%)\n",
      "  memory             170Mi (1%)  170Mi (1%)\n",
      "  ephemeral-storage  100Mi (0%)  0 (0%)\n",
      "  hugepages-1Gi      0 (0%)      0 (0%)\n",
      "  hugepages-2Mi      0 (0%)      0 (0%)\n",
      "Events:\n",
      "  Type    Reason                   Age                    From        Message\n",
      "  ----    ------                   ----                   ----        -------\n",
      "  Normal  Starting                 6m37s                  kubelet     Starting kubelet.\n",
      "  Normal  NodeAllocatableEnforced  6m37s                  kubelet     Updated Node Allocatable limit across pods\n",
      "  Normal  NodeHasSufficientMemory  6m36s (x4 over 6m37s)  kubelet     Node 442be9fcde81 status is now: NodeHasSufficientMemory\n",
      "  Normal  NodeHasNoDiskPressure    6m36s (x4 over 6m37s)  kubelet     Node 442be9fcde81 status is now: NodeHasNoDiskPressure\n",
      "  Normal  NodeHasSufficientPID     6m36s (x3 over 6m37s)  kubelet     Node 442be9fcde81 status is now: NodeHasSufficientPID\n",
      "  Normal  Starting                 5m58s                  kubelet     Starting kubelet.\n",
      "  Normal  NodeHasSufficientMemory  5m58s                  kubelet     Node 442be9fcde81 status is now: NodeHasSufficientMemory\n",
      "  Normal  NodeHasNoDiskPressure    5m58s                  kubelet     Node 442be9fcde81 status is now: NodeHasNoDiskPressure\n",
      "  Normal  NodeHasSufficientPID     5m58s                  kubelet     Node 442be9fcde81 status is now: NodeHasSufficientPID\n",
      "  Normal  NodeAllocatableEnforced  5m57s                  kubelet     Updated Node Allocatable limit across pods\n",
      "  Normal  NodeReady                5m47s                  kubelet     Node 442be9fcde81 status is now: NodeReady\n",
      "  Normal  Starting                 5m45s                  kube-proxy  Starting kube-proxy.\n"
     ]
    }
   ],
   "source": [
    "# Can we see the GPU?\n",
    "!kubectl describe nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a08be96",
   "metadata": {},
   "source": [
    "Can you find anything? Try again, filtering the output with `grep`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d73b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look for the lines containing the word \"nvidia\"\n",
    "!kubectl describe nodes | grep nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd74565",
   "metadata": {},
   "source": [
    "We did not find anything. That would explain why the application is still pending. Our cluster is not aware of the presence of the GPU.  The cluster is unable to schedule the execution since our YAML required GPU resources, but they are for all intents and purposes unavailable. We need to add the NVIDIA GPU device plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd86c369",
   "metadata": {},
   "source": [
    "--- \n",
    "# 9.3 Add GPU Awareness to K8s\n",
    "To take advantage of GPU acceleration on Kubernetes, install the [NVIDIA GPU plugin](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/#deploying-nvidia-gpu-device-plugin) to the cluster. Before adding it, look at the status without the plugin  with `kubectl get`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb26e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE\n",
      "default       gpu-operator-test                      0/1     Pending   0          2m56s\n",
      "kube-system   coredns-74ff55c5b-zv7v2                1/1     Running   0          7m11s\n",
      "kube-system   etcd-442be9fcde81                      1/1     Running   0          7m22s\n",
      "kube-system   kube-apiserver-442be9fcde81            1/1     Running   0          7m22s\n",
      "kube-system   kube-controller-manager-442be9fcde81   1/1     Running   0          7m22s\n",
      "kube-system   kube-proxy-n6m24                       1/1     Running   0          7m11s\n",
      "kube-system   kube-scheduler-442be9fcde81            1/1     Running   0          7m22s\n",
      "kube-system   storage-provisioner                    1/1     Running   0          7m25s\n"
     ]
    }
   ],
   "source": [
    "# Try to find the GPU device plugin. Not there \n",
    "!kubectl get pods -A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a857e7ff",
   "metadata": {},
   "source": [
    "To install the NVIDIA GPU plugin, we can use the Kubernetes package manager [Helm](https://helm.sh/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d1c816a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"nvdp\" has been added to your repositories\n",
      "Hang tight while we grab the latest from your chart repositories...\n",
      "...Successfully got an update from the \"nvdp\" chart repository\n",
      "Update Complete. ⎈Happy Helming!⎈\n",
      "NAME: nvidia-device-plugin-1651056784\n",
      "LAST DEPLOYED: Wed Apr 27 10:53:04 2022\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "# Install the device plugin with the Helm package manager\n",
    "!helm repo add nvdp https://nvidia.github.io/k8s-device-plugin \\\n",
    "   && helm repo update\n",
    "!helm install --version=0.9.0 --generate-name nvdp/nvidia-device-plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4202eda",
   "metadata": {},
   "source": [
    "Check the status again to make sure the plugin was deployed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c27e1db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                    READY   STATUS      RESTARTS   AGE\n",
      "default       gpu-operator-test                       0/1     Completed   0          4m33s\n",
      "kube-system   coredns-74ff55c5b-zv7v2                 1/1     Running     0          8m48s\n",
      "kube-system   etcd-442be9fcde81                       1/1     Running     0          8m59s\n",
      "kube-system   kube-apiserver-442be9fcde81             1/1     Running     0          8m59s\n",
      "kube-system   kube-controller-manager-442be9fcde81    1/1     Running     0          8m59s\n",
      "kube-system   kube-proxy-n6m24                        1/1     Running     0          8m48s\n",
      "kube-system   kube-scheduler-442be9fcde81             1/1     Running     0          8m59s\n",
      "kube-system   nvidia-device-plugin-1651056784-z2zj2   1/1     Running     0          29s\n",
      "kube-system   storage-provisioner                     1/1     Running     0          9m2s\n"
     ]
    }
   ],
   "source": [
    "# Now the device plugin \"nvidia-device-plugin-*\" should be \"Running\" after a \"ContainerCreating\" status\n",
    "!kubectl get pods -A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e7ae5",
   "metadata": {},
   "source": [
    "We should now see the NVIDIA-specific configuration listed against the nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52980454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:               442be9fcde81\n",
      "Roles:              control-plane,master\n",
      "Labels:             beta.kubernetes.io/arch=amd64\n",
      "                    beta.kubernetes.io/os=linux\n",
      "                    kubernetes.io/arch=amd64\n",
      "                    kubernetes.io/hostname=442be9fcde81\n",
      "                    kubernetes.io/os=linux\n",
      "                    minikube.k8s.io/commit=15cede53bdc5fe242228853e737333b09d4336b5\n",
      "                    minikube.k8s.io/name=minikube\n",
      "                    minikube.k8s.io/updated_at=2022_04_27T10_44_30_0700\n",
      "                    minikube.k8s.io/version=v1.19.0\n",
      "                    node-role.kubernetes.io/control-plane=\n",
      "                    node-role.kubernetes.io/master=\n",
      "Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n",
      "                    node.alpha.kubernetes.io/ttl: 0\n",
      "                    volumes.kubernetes.io/controller-managed-attach-detach: true\n",
      "CreationTimestamp:  Wed, 27 Apr 2022 10:44:29 +0000\n",
      "Taints:             <none>\n",
      "Unschedulable:      false\n",
      "Lease:\n",
      "  HolderIdentity:  442be9fcde81\n",
      "  AcquireTime:     <unset>\n",
      "  RenewTime:       Wed, 27 Apr 2022 10:53:54 +0000\n",
      "Conditions:\n",
      "  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n",
      "  ----             ------  -----------------                 ------------------                ------                       -------\n",
      "  MemoryPressure   False   Wed, 27 Apr 2022 10:53:37 +0000   Wed, 27 Apr 2022 10:44:28 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n",
      "  DiskPressure     False   Wed, 27 Apr 2022 10:53:37 +0000   Wed, 27 Apr 2022 10:44:28 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n",
      "  PIDPressure      False   Wed, 27 Apr 2022 10:53:37 +0000   Wed, 27 Apr 2022 10:44:28 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n",
      "  Ready            True    Wed, 27 Apr 2022 10:53:37 +0000   Wed, 27 Apr 2022 10:44:44 +0000   KubeletReady                 kubelet is posting ready status\n",
      "Addresses:\n",
      "  InternalIP:  172.18.0.5\n",
      "  Hostname:    442be9fcde81\n",
      "Capacity:\n",
      "  cpu:                4\n",
      "  ephemeral-storage:  304884524Ki\n",
      "  hugepages-1Gi:      0\n",
      "  hugepages-2Mi:      0\n",
      "  memory:             16095212Ki\n",
      "  nvidia.com/gpu:     1\n",
      "  pods:               110\n",
      "Allocatable:\n",
      "  cpu:                4\n",
      "  ephemeral-storage:  304884524Ki\n",
      "  hugepages-1Gi:      0\n",
      "  hugepages-2Mi:      0\n",
      "  memory:             16095212Ki\n",
      "  nvidia.com/gpu:     1\n",
      "  pods:               110\n",
      "System Info:\n",
      "  Machine ID:                 84fb46bd39d2483a97ab4430ee4a5e3a\n",
      "  System UUID:                4138acd6-eadc-4c1a-8c42-4fbbc960846d\n",
      "  Boot ID:                    a3589ab0-527a-424b-aa63-1ab9fd222aca\n",
      "  Kernel Version:             5.4.0-1041-aws\n",
      "  OS Image:                   Ubuntu 20.04.1 LTS\n",
      "  Operating System:           linux\n",
      "  Architecture:               amd64\n",
      "  Container Runtime Version:  docker://20.10.3\n",
      "  Kubelet Version:            v1.20.2\n",
      "  Kube-Proxy Version:         v1.20.2\n",
      "PodCIDR:                      10.244.0.0/24\n",
      "PodCIDRs:                     10.244.0.0/24\n",
      "Non-terminated Pods:          (8 in total)\n",
      "  Namespace                   Name                                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n",
      "  ---------                   ----                                     ------------  ----------  ---------------  -------------  ---\n",
      "  kube-system                 coredns-74ff55c5b-zv7v2                  100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     9m11s\n",
      "  kube-system                 etcd-442be9fcde81                        100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         9m22s\n",
      "  kube-system                 kube-apiserver-442be9fcde81              250m (6%)     0 (0%)      0 (0%)           0 (0%)         9m22s\n",
      "  kube-system                 kube-controller-manager-442be9fcde81     200m (5%)     0 (0%)      0 (0%)           0 (0%)         9m22s\n",
      "  kube-system                 kube-proxy-n6m24                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m11s\n",
      "  kube-system                 kube-scheduler-442be9fcde81              100m (2%)     0 (0%)      0 (0%)           0 (0%)         9m22s\n",
      "  kube-system                 nvidia-device-plugin-1651056784-z2zj2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         52s\n",
      "  kube-system                 storage-provisioner                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m25s\n",
      "Allocated resources:\n",
      "  (Total limits may be over 100 percent, i.e., overcommitted.)\n",
      "  Resource           Requests    Limits\n",
      "  --------           --------    ------\n",
      "  cpu                750m (18%)  0 (0%)\n",
      "  memory             170Mi (1%)  170Mi (1%)\n",
      "  ephemeral-storage  100Mi (0%)  0 (0%)\n",
      "  hugepages-1Gi      0 (0%)      0 (0%)\n",
      "  hugepages-2Mi      0 (0%)      0 (0%)\n",
      "  nvidia.com/gpu     0           0\n",
      "Events:\n",
      "  Type    Reason                   Age                From        Message\n",
      "  ----    ------                   ----               ----        -------\n",
      "  Normal  Starting                 10m                kubelet     Starting kubelet.\n",
      "  Normal  NodeAllocatableEnforced  10m                kubelet     Updated Node Allocatable limit across pods\n",
      "  Normal  NodeHasSufficientMemory  10m (x4 over 10m)  kubelet     Node 442be9fcde81 status is now: NodeHasSufficientMemory\n",
      "  Normal  NodeHasNoDiskPressure    10m (x4 over 10m)  kubelet     Node 442be9fcde81 status is now: NodeHasNoDiskPressure\n",
      "  Normal  NodeHasSufficientPID     10m (x3 over 10m)  kubelet     Node 442be9fcde81 status is now: NodeHasSufficientPID\n",
      "  Normal  Starting                 9m23s              kubelet     Starting kubelet.\n",
      "  Normal  NodeHasSufficientMemory  9m23s              kubelet     Node 442be9fcde81 status is now: NodeHasSufficientMemory\n",
      "  Normal  NodeHasNoDiskPressure    9m23s              kubelet     Node 442be9fcde81 status is now: NodeHasNoDiskPressure\n",
      "  Normal  NodeHasSufficientPID     9m23s              kubelet     Node 442be9fcde81 status is now: NodeHasSufficientPID\n",
      "  Normal  NodeAllocatableEnforced  9m22s              kubelet     Updated Node Allocatable limit across pods\n",
      "  Normal  NodeReady                9m12s              kubelet     Node 442be9fcde81 status is now: NodeReady\n",
      "  Normal  Starting                 9m10s              kube-proxy  Starting kube-proxy.\n"
     ]
    }
   ],
   "source": [
    "# Now we should see Allocable GPUs\n",
    "!kubectl describe nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6623709a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  nvidia.com/gpu:     1\n",
      "  nvidia.com/gpu:     1\n",
      "  kube-system                 nvidia-device-plugin-1651056784-z2zj2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         76s\n",
      "  nvidia.com/gpu     0           0\n"
     ]
    }
   ],
   "source": [
    "# Let's look for the lines containing the word nvidia\n",
    "!kubectl describe nodes | grep nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23857f",
   "metadata": {},
   "source": [
    "As we deployed the GPU device plugin, what do you think happened to our application?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8089ed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                READY   STATUS      RESTARTS   AGE\n",
      "gpu-operator-test   0/1     Completed   0          5m33s\n"
     ]
    }
   ],
   "source": [
    "# Let's check the application again\n",
    "!kubectl get pods gpu-operator-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d924660a",
   "metadata": {},
   "source": [
    "Our application executed successfully when the GPU resources became available. In fact, it has now completed so we can have a look at its execution logs with `kubectl logs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1771de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vector addition of 50000 elements]\n",
      "Copy input data from the host memory to the CUDA device\n",
      "CUDA kernel launch with 196 blocks of 256 threads\n",
      "Copy output data from the CUDA device to the host memory\n",
      "Test PASSED\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the output\n",
    "!kubectl logs gpu-operator-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af3a04",
   "metadata": {},
   "source": [
    "Check the list of Helm charts installed with the `helm list` command (see the [Helm documentation](https://helm.sh/docs/helm/helm_list/)). The `--filter` option allows filtering by name.  Use the `--output` option to specify the output format (\"json\", \"table\", or \"yaml\").  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec52e6d",
   "metadata": {},
   "source": [
    "Now, let's delete the Kubernetes pod `gpu-operator-test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5460dd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"gpu-operator-test\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Let's delete the pod\n",
    "!kubectl delete pod gpu-operator-test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2762b87",
   "metadata": {},
   "source": [
    "Congratulations! You deployed a GPU accelerated applicaiton with Kuberenetes. So far, we have specified that we want a single GPU without specifying which GPU we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d8d11e",
   "metadata": {},
   "source": [
    "--- \n",
    "# 9.4 Interact with GPU Resources in K8s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f58e7",
   "metadata": {},
   "source": [
    "Now, let's see how to get more control over the GPU-accelerated cluster. Being able to control the GPU type, or the MIG ([Multi-Instance GPU](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/)) partition on an Ampere GPU is very important as GPUs vary in terms of computational capability, memory, and cost. The MIG allows users to fragment the GPU into as many as 7 (on A100) partitions. This allows more granular control over the resources in the cluster and better application isolation. \n",
    "\n",
    "In order to control the GPU type, we'll add the `gpu-feature-discovery` plugin and deploy it with Helm. This plugin can be configured with several options, as described in the [gpu-feature-discovery repository](https://github.com/NVIDIA/gpu-feature-discovery#deployment-via-helm). One of the most interesting options when working with Ampere GPUs is the ability to support MIG partitions. The feature discovery plugin can be deployed with the following configurable features:\n",
    "\n",
    "\n",
    "|Feature|Description|Default|\n",
    "|-|-|-|\n",
    "|`failOnInitError`|Fail if there is an error during initialization of any label sources|\"true\"|\n",
    "|`sleepInterval`|Time to sleep between labeling|\"60s\"|\n",
    "|`migStrategy`|Pass the desired strategy for labeling MIG devices on GPUs that support it [none | single | mixed]|\"none\"|\n",
    "|`nfd.deploy`|When set to true, deploy NFD as a subchart with all of the proper parameters set for it|\"true\"|\n",
    "\n",
    "In this class, we are not using Ampere GPUs, so we will do a simple install:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8673b7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"nvgfd\" has been added to your repositories\n",
      "Hang tight while we grab the latest from your chart repositories...\n",
      "...Successfully got an update from the \"nvgfd\" chart repository\n",
      "...Successfully got an update from the \"nvdp\" chart repository\n",
      "Update Complete. ⎈Happy Helming!⎈\n",
      "NAME: gpu-feature-discovery-1651057196\n",
      "LAST DEPLOYED: Wed Apr 27 10:59:56 2022\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm repo add nvgfd https://nvidia.github.io/gpu-feature-discovery \\\n",
    "    && helm repo update\n",
    "!helm install \\\n",
    "    --version=0.4.1 \\\n",
    "    --generate-name \\\n",
    "    nvgfd/gpu-feature-discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0685b",
   "metadata": {},
   "source": [
    "Let's look at additional information that we have about our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bca565a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  nvidia.com/gpu:     1\n",
      "  pods:               110\n",
      "Allocatable:\n",
      "  cpu:                4\n",
      "  ephemeral-storage:  304884524Ki\n",
      "  hugepages-1Gi:      0\n",
      "  hugepages-2Mi:      0\n",
      "  memory:             16095212Ki\n",
      "  nvidia.com/gpu:     1\n",
      "  pods:               110\n",
      "System Info:\n",
      "  Machine ID:                 84fb46bd39d2483a97ab4430ee4a5e3a\n",
      "  System UUID:                4138acd6-eadc-4c1a-8c42-4fbbc960846d\n",
      "  Boot ID:                    a3589ab0-527a-424b-aa63-1ab9fd222aca\n",
      "  Kernel Version:             5.4.0-1041-aws\n",
      "  OS Image:                   Ubuntu 20.04.1 LTS\n",
      "  Operating System:           linux\n",
      "  Architecture:               amd64\n",
      "  Container Runtime Version:  docker://20.10.3\n",
      "  Kubelet Version:            v1.20.2\n",
      "  Kube-Proxy Version:         v1.20.2\n",
      "PodCIDR:                      10.244.0.0/24\n",
      "PodCIDRs:                     10.244.0.0/24\n",
      "Non-terminated Pods:          (11 in total)\n",
      "--\n",
      "  nvidia.com/gpu     0           0\n",
      "Events:\n",
      "  Type    Reason                   Age                From        Message\n",
      "  ----    ------                   ----               ----        -------\n",
      "  Normal  Starting                 16m                kubelet     Starting kubelet.\n",
      "  Normal  NodeAllocatableEnforced  16m                kubelet     Updated Node Allocatable limit across pods\n",
      "  Normal  NodeHasSufficientMemory  16m (x4 over 16m)  kubelet     Node 442be9fcde81 status is now: NodeHasSufficientMemory\n",
      "  Normal  NodeHasNoDiskPressure    16m (x4 over 16m)  kubelet     Node 442be9fcde81 status is now: NodeHasNoDiskPressure\n",
      "  Normal  NodeHasSufficientPID     16m (x3 over 16m)  kubelet     Node 442be9fcde81 status is now: NodeHasSufficientPID\n",
      "  Normal  Starting                 15m                kubelet     Starting kubelet.\n",
      "  Normal  NodeHasSufficientMemory  15m                kubelet     Node 442be9fcde81 status is now: NodeHasSufficientMemory\n",
      "  Normal  NodeHasNoDiskPressure    15m                kubelet     Node 442be9fcde81 status is now: NodeHasNoDiskPressure\n",
      "  Normal  NodeHasSufficientPID     15m                kubelet     Node 442be9fcde81 status is now: NodeHasSufficientPID\n",
      "  Normal  NodeAllocatableEnforced  15m                kubelet     Updated Node Allocatable limit across pods\n",
      "  Normal  NodeReady                15m                kubelet     Node 442be9fcde81 status is now: NodeReady\n",
      "  Normal  Starting                 15m                kube-proxy  Starting kube-proxy.\n"
     ]
    }
   ],
   "source": [
    "# Looking for all of the NVIDIA related information\n",
    "!kubectl describe nodes | grep \"nvidia.com\" -A 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de54fbc",
   "metadata": {},
   "source": [
    "You should see a wide range of GPU-specific information, including the driver and CUDA information, as well as which GPU is in use from `nvidia.com/gpu.product`.\n",
    "\n",
    "This is probably a Tesla-T4, unless you are running the class on an alternative GPU. Recall that we deployed our test application `gpu-operator-test` with a generic \"GPU\".  It is possible to deploy it with more specific information regarding the GPU. \n",
    "\n",
    "A new YAML file, `gpu-pod-T4.yaml`, is already prepared. Let's inspect it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "077b5b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: gpu-operator-test-a100\n",
      "spec:\n",
      "  restartPolicy: OnFailure\n",
      "  containers:\n",
      "  - name: cuda-vector-add\n",
      "    image: \"nvidia/samples:vectoradd-cuda10.2\"\n",
      "    resources:\n",
      "      limits:\n",
      "         nvidia.com/gpu: 1\n",
      "  nodeSelector: \n",
      "    nvidia.com/gpu.product: A100-SXM4-40GB"
     ]
    }
   ],
   "source": [
    "# Review the application we are deploying\n",
    "!cat $CONFIG_DIR/gpu-pod-T4.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab0bd2",
   "metadata": {},
   "source": [
    "As you might have noticed, the YAML was configured to deploy on an A100 GPU, which is not available in the class. Go ahead and deploy the application anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b13f94db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/gpu-operator-test-a100 created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f $CONFIG_DIR/gpu-pod-T4.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12ef833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                     READY   STATUS    RESTARTS   AGE\n",
      "gpu-operator-test-a100   0/1     Pending   0          8s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods gpu-operator-test-a100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06764b1b",
   "metadata": {},
   "source": [
    "Just as we saw in the earlier non-GPU case, the deployment is in the \"Pending\" state and it will remain in this state until an A100 GPU becomes available or it is terminated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11678db5",
   "metadata": {},
   "source": [
    "## 9.4.1 Exercise: Configure Pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04459caf",
   "metadata": {},
   "source": [
    "Modify the YAML file and deploy the `gpu-operator-test` application on the correct GPU.\n",
    "Open the [gpu-pod-T4.yaml](kubernetes-config/gpu-pod-T4.yaml) config file and make those chages:\n",
    "* Change the pod name to \"gpu-operator-test-t4\"\n",
    "* Set the GPU product to \"Tesla-T4\" instead of the A100\n",
    "\n",
    "Check your work against the [solution](solutions/ex9.4.1.yaml) before moving on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8054f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO modify gpu-pod-T4.yaml so that this cell verifies changes are correct\n",
    "# Check your work - you'll get no output if the files match\n",
    "!diff $CONFIG_DIR/gpu-pod-T4.yaml solutions/ex9.4.1.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78df9da",
   "metadata": {},
   "source": [
    "Next, deploy the `gpu-operator-test-t4` pod using the modified [gpu-pod-T4.yaml](kubernetes-config/gpu-pod-T4.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e849092e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/gpu-operator-test-t4 created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f $CONFIG_DIR/gpu-pod-T4.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b7c42",
   "metadata": {},
   "source": [
    "## 9.4.2 Final Checks and Shutdown\n",
    "It might take a few seconds, but the application should deploy and finish successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3a9f5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   READY   STATUS      RESTARTS   AGE\n",
      "gpu-operator-test-t4   0/1     Completed   0          12s\n"
     ]
    }
   ],
   "source": [
    "# Get the status of the pod deployed\n",
    "!kubectl get pods gpu-operator-test-t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d40288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vector addition of 50000 elements]\n",
      "Copy input data from the host memory to the CUDA device\n",
      "CUDA kernel launch with 196 blocks of 256 threads\n",
      "Copy output data from the CUDA device to the host memory\n",
      "Test PASSED\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the output\n",
    "!kubectl logs gpu-operator-test-t4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b53bef",
   "metadata": {},
   "source": [
    "### 9.4.2.1 Exercise: Delete a Pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b5845",
   "metadata": {},
   "source": [
    "Delete the Kubernetes pod `gpu-operator-test-t4`. Check the [solution](solutions/ex9.4.2.ipynb) before moving on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "edc76e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"gpu-operator-test-t4\" deleted\n"
     ]
    }
   ],
   "source": [
    "# TODO delete the pod\n",
    "!kubectl delete pod gpu-operator-test-t4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31c45c",
   "metadata": {},
   "source": [
    "Before moving forward to the next notebook, shut down K8s and clean up the docker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "972fcc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄  Uninstalling Kubernetes v1.20.2 using kubeadm ...\n",
      "🔥  Deleting \"minikube\" in none ...\n",
      "💀  Removed all traces of the \"minikube\" cluster.\n",
      "\"docker kill\" requires at least 1 argument.\n",
      "See 'docker kill --help'.\n",
      "\n",
      "Usage:  docker kill [OPTIONS] CONTAINER [CONTAINER...]\n",
      "\n",
      "Kill one or more running containers\n",
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "# Shut down K8s\n",
    "!minikube delete\n",
    "# Shut down running docker containers\n",
    "!docker kill $(docker ps -q)\n",
    "# Check for clean environment - this should be empty\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c1692e",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you have:\n",
    "- Launched a K8s cluster\n",
    "- Interacted with K8s using `kubectl`\n",
    "- Installed plugins with Helm\n",
    "- Enabled GPU acceleration and GPU feature discovery\n",
    "- Deployed an application\n",
    "\n",
    "Next, you'll monitor activity on the cluster. Move on to [Monitoring GPU within Kubernetes Cluster](010_K8s_Monitor.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da44ae1",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
